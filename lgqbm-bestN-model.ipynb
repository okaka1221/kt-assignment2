{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "import operator\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "import string\n",
    "import re\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train_tweets.csv')\n",
    "dev = pd.read_csv('./data/dev_tweets.csv')\n",
    "test = pd.read_csv('./data/test_tweets.csv')\n",
    "\n",
    "train['tweet'] = train['tweet'].astype(str)\n",
    "dev['tweet'] = dev['tweet'].astype(str)\n",
    "test['tweet'] = test['tweet'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (96585, 4)\n",
      "Dev shape  : (34028, 4)\n",
      "Test shape : (32977, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Train shape:', train.shape)\n",
    "print('Dev shape  :', dev.shape)\n",
    "print('Test shape :', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for special in specials:\n",
    "        text = text.replace(special, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(' ')])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove '@USER_*****'\n",
    "train['tweet'] = train['tweet'].apply(lambda x: ' '.join([word for word in x.split(' ') if word[0:6] != '@USER_']))\n",
    "dev['tweet'] = dev['tweet'].apply(lambda x: ' '.join([word for word in x.split(' ') if word[0:6] != '@USER_']))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: ' '.join([word for word in x.split(' ') if word[0:6] != '@USER_']))\n",
    "\n",
    "# clean contruction\n",
    "train['tweet'] = train['tweet'].apply(lambda x: clean_contractions(x, contraction_mapping))\n",
    "dev['tweet'] = dev['tweet'].apply(lambda x: clean_contractions(x, contraction_mapping))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: clean_contractions(x, contraction_mapping))\n",
    "\n",
    "# lower text\n",
    "train['tweet'] = train['tweet'].apply(lambda x: x.lower())\n",
    "dev['tweet'] = dev['tweet'].apply(lambda x: x.lower())\n",
    "test['tweet'] = test['tweet'].apply(lambda x: x.lower())\n",
    "\n",
    "# remove stop words\n",
    "train['tweet'] = train['tweet'].apply(lambda x: ' '.join([word for word in x.split(' ') if word not in stopwords]))\n",
    "dev['tweet'] = dev['tweet'].apply(lambda x: ' '.join([word for word in x.split(' ') if word not in stopwords]))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: ' '.join([word for word in x.split(' ') if word not in stopwords]))\n",
    "\n",
    "# remove non alphabetic chars\n",
    "train['tweet'] = train['tweet'].apply(lambda x: re.sub('[^a-z]+', ' ', x))\n",
    "dev['tweet'] = dev['tweet'].apply(lambda x: re.sub('[^a-z]+', ' ', x))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: re.sub('[^a-z]+', ' ', x))\n",
    "\n",
    "# strip text\n",
    "train['tweet'] = train['tweet'].apply(lambda x: x.strip())\n",
    "dev['tweet'] = dev['tweet'].apply(lambda x: x.strip())\n",
    "test['tweet'] = test['tweet'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_same_user(df):\n",
    "    df_class = df[['user-id', 'class']].groupby('user-id').apply(lambda x: x['class'].mode()).reset_index()\n",
    "    df_class = df_class.rename(columns={0: 'class'})\n",
    "    df_new = df[['user-id', 'tweet']].groupby('user-id').agg(lambda x: ' '.join(x))\n",
    "    df_new = pd.merge(df_new, df_class, on='user-id')\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_per_user = merge_same_user(train)\n",
    "dev_per_user = merge_same_user(dev)\n",
    "test_per_user = merge_same_user(test)\n",
    "\n",
    "train_per_user = pd.concat([train_per_user, dev_per_user], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train per user shape: (3190, 3)\n",
      "Test per user shape : (802, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Train per user shape:', train_per_user.shape)\n",
    "print('Test per user shape :', test_per_user.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = np.load(f'best{N}.npy')\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=word_list)\n",
    "vectorizer.fit(train_per_user['tweet'])\n",
    "\n",
    "train_vec = vectorizer.transform(train_per_user['tweet'])\n",
    "test_vec = vectorizer.transform(test_per_user['tweet'])\n",
    "\n",
    "train_vec = pd.DataFrame(train_vec.todense(), columns=vectorizer.get_feature_names())\n",
    "test_vec = pd.DataFrame(test_vec.todense(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "train_per_user = train_per_user.drop(['tweet'], axis=1)\n",
    "test_per_user = test_per_user.drop(['tweet'], axis=1)\n",
    "\n",
    "train_per_user = train_per_user.reset_index(drop=True)\n",
    "test_per_user = test_per_user.reset_index(drop=True)\n",
    "train_vec = train_vec.reset_index(drop=True)\n",
    "test_vec = test_vec.reset_index(drop=True)\n",
    "\n",
    "train_per_user = pd.concat([train_per_user, train_vec], axis=1)\n",
    "test_per_user = pd.concat([test_per_user, test_vec], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train per user shape: (3190, 972)\n",
      "Test per user shape : (802, 972)\n"
     ]
    }
   ],
   "source": [
    "print('Train per user shape:', train_per_user.shape)\n",
    "print('Test per user shape :', test_per_user.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(data, pred):\n",
    "    y_true = data\n",
    "    reshaped_pred = pred.reshape(3, len(pred) // 3)\n",
    "    y_pred = np.argmax(reshaped_pred, axis=0)\n",
    "    acc = np.mean(y_true == y_pred)\n",
    "    return 'accuracy', acc, True\n",
    "\n",
    "def train_model(X, y, X_test, params, fold_num):\n",
    "    result_dict = {}\n",
    "    oof = np.zeros((len(X), 3))\n",
    "    prediction = np.zeros((len(X_test), 3))\n",
    "    scores = []\n",
    "    features = X.columns\n",
    "    feature_importance = pd.DataFrame()\n",
    "\n",
    "    folds = StratifiedKFold(n_splits=fold_num)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "        print(f'\\nFold {fold + 1} started at {time.ctime()}')\n",
    "        \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(\n",
    "            **params,\n",
    "            )\n",
    "        \n",
    "        model.fit(X_train, y_train,\n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  eval_metric=accuracy,\n",
    "                  verbose=False,\n",
    "                  early_stopping_rounds=100)\n",
    "        \n",
    "        y_pred_val_proba = model.predict_proba(X_val)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        \n",
    "        accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "        scores.append(accuracy_val)\n",
    "        \n",
    "        print('\\nFold {0: d} accuracy: {1: .4f}'.format(fold + 1, accuracy_val))\n",
    "        \n",
    "        oof[val_idx] += y_pred_val_proba.reshape(-1, 3)\n",
    "        prediction += y_pred_proba / fold_num\n",
    "        \n",
    "        fold_importance = pd.DataFrame()\n",
    "        fold_importance['feature'] = features\n",
    "        fold_importance['importance'] = model.feature_importances_\n",
    "        fold_importance['fold'] = fold + 1\n",
    "        feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "        \n",
    "    print('\\nCV mean accuracy {0: .4f}'.format(np.mean(scores)))\n",
    "    \n",
    "    cols = feature_importance[['feature', 'importance']].groupby('feature').mean().sort_values(by='importance', ascending=False).index\n",
    "    best_features = feature_importance[['feature', 'importance']].groupby('feature').mean().sort_values(by='importance', ascending=False).reset_index()\n",
    "\n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    result_dict['scores'] = scores\n",
    "    result_dict['feature_importance'] = best_features\n",
    "    \n",
    "#     plt.figure(figsize=(16,128))\n",
    "#     sns.barplot(x='importance', y='feature', data=best_features)\n",
    "#     plt.title('LGB Features (avg over folds)')\n",
    "        \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 started at Sun Oct 13 09:49:38 2019\n",
      "\n",
      "Fold  1 accuracy:  0.8287\n",
      "\n",
      "Fold 2 started at Sun Oct 13 09:49:51 2019\n",
      "\n",
      "Fold  2 accuracy:  0.8349\n",
      "\n",
      "Fold 3 started at Sun Oct 13 09:50:03 2019\n",
      "\n",
      "Fold  3 accuracy:  0.8094\n",
      "\n",
      "Fold 4 started at Sun Oct 13 09:50:15 2019\n",
      "\n",
      "Fold  4 accuracy:  0.8219\n",
      "\n",
      "Fold 5 started at Sun Oct 13 09:50:30 2019\n",
      "\n",
      "Fold  5 accuracy:  0.8428\n",
      "\n",
      "Fold 6 started at Sun Oct 13 09:50:43 2019\n",
      "\n",
      "Fold  6 accuracy:  0.8239\n",
      "\n",
      "Fold 7 started at Sun Oct 13 09:50:53 2019\n",
      "\n",
      "Fold  7 accuracy:  0.8019\n",
      "\n",
      "Fold 8 started at Sun Oct 13 09:51:09 2019\n",
      "\n",
      "Fold  8 accuracy:  0.8302\n",
      "\n",
      "Fold 9 started at Sun Oct 13 09:51:22 2019\n",
      "\n",
      "Fold  9 accuracy:  0.8459\n",
      "\n",
      "Fold 10 started at Sun Oct 13 09:51:32 2019\n",
      "\n",
      "Fold  10 accuracy:  0.8365\n",
      "\n",
      "CV mean accuracy  0.8276\n"
     ]
    }
   ],
   "source": [
    "X = train_per_user.drop(['user-id', 'class'], axis=1)\n",
    "X_test = test_per_user.drop(['user-id', 'class'], axis=1)\n",
    "\n",
    "y = train_per_user['class']\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "params = {\n",
    "    'n_estimators': 5000,\n",
    "}\n",
    "\n",
    "result_dict = train_model(X, y, X_test, params, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8275862068965517"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = train_per_user[['user-id', 'class']]\n",
    "validation['prediction'] = encoder.inverse_transform(np.argmax(result_dict['oof'], axis=1))\n",
    "validation[encoder.classes_[0]] = result_dict['oof'][:, 0]\n",
    "validation[encoder.classes_[1]] = result_dict['oof'][:, 1]\n",
    "validation[encoder.classes_[2]] = result_dict['oof'][:, 2]\n",
    "validation.to_csv(f'./result-per-user/lgbm-best{N}-val.csv', index=False)\n",
    "accuracy_score(validation['class'], validation['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test_per_user[['user-id', 'class']]\n",
    "submission['prediction'] = encoder.inverse_transform(np.argmax(result_dict['prediction'], axis=1))\n",
    "submission[encoder.classes_[0]] = result_dict['prediction'][:, 0]\n",
    "submission[encoder.classes_[1]] = result_dict['prediction'][:, 1]\n",
    "submission[encoder.classes_[2]] = result_dict['prediction'][:, 2]\n",
    "submission.to_csv(f'./result-per-user/lgbm-best{N}-sub.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
